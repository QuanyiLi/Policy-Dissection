<!doctype html>
<html lang="en">

<!-- === Header Starts === -->
<head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
    <title> Human-AI Shared Control via Policy Dissection</title>
    <link href="./assets/bootstrap.min.css" rel="stylesheet">
    <link href="./assets/font.css" rel="stylesheet" type="text/css">
    <link href="./assets/style.css" rel="stylesheet" type="text/css">
    <script src="./assets/jquery.min.js"></script>
    <script type="text/javascript" src="assets/corpus.js"></script>

</head>
<!-- === Header Ends === -->

<script>
    var lang_flag = 1;


</script>

<body>

<!-- === Home Section Starts === -->
<div class="section">
    <!-- === Title Starts === -->
    <div class="header">
        <!--        <div class="logo">-->
        <!--            <a href="https://decisionforce.github.io/" target="_blank">-->
        <!--                <img src="images/deciforce.png">-->
        <!--            </a>-->
        <!--        </div>-->
        <!--        <div style="padding-top: 30pt; margin: 0 50pt;" class="title" id="lang">-->
        <!--            Safe Driving via Expert Guided Policy Optimization-->
        <!--        </div>-->

        <table>
            <tr>
                <td>
                    <!--
                                        <div class="logo"
                                             style="
                                             width: 100pt;
                                             vertical-align: text-top;
                                             text-align: center;
                                             ">
                                            <a href="https://decisionforce.github.io/" target="_blank">
                                                <img src="images/deciforce.png">
                                            </a>
                                        </div>-->

                </td>
                <td>
                    <div style="padding-top: 0pt;padding-left: 95pt;padding-bottom: 10pt" class="title" id="lang">
                        Human-AI Shared Control via Policy Dissection
                    </div>
                    <p style="text-align:center;padding-left: 90pt">Neural Information Processing Systems (NeurIPS)
                        2022</p>
                </td>
                <td>
                    <!--
                                        <div class="logo"
                                             style="
                                             width: 100pt;
                                             padding-top: 10pt;
                                             vertical-align: center;
                                             text-align: center;
                                             ">
                                            <a href="https://github.com/decisionforce/metadrive" target="_blank">
                                                <img style=" width: 120pt;" src="images/metadrive.png">
                                            </a>
                                        </div>
                    -->
                </td>
            </tr>
        </table>


    </div>
    <!-- === Title Ends === -->
    <div class="author">
        <a href="https://Quanyili.github.io">Quanyi Li</a><sup>1,4</sup>,&nbsp;
        <a href="https://pengzhenghao.github.io" target="_blank">Zhenghao Peng</a><sup>3</sup>,&nbsp;&nbsp;
        <a href="https://hbwu-ntu.github.io/" target="_blank">Haibin Wu</a><sup>1</sup>,
        <a href="#" target="_blank">Lan Feng</a><sup>2</sup>,
        <a href="https://boleizhou.github.io/" target="_blank">Bolei Zhou</a><sup>3</sup>&nbsp;

    </div>

    <div class="institution" style="font-size: 11pt;">
        <div>
            <sup>1</sup>Centre for Perceptual and Interactive Intelligence,
            <sup>2</sup>ETH Zurich, <br>
            <sup>3</sup>University of California, Los Angeles
            <sup>4</sup>University of Edinburgh<br>

        </div>
    </div>
    <table border="0" align="center">
        <tr>
            <td align="center" style="padding: 0pt 0 15pt 0">
                <a class="bar" href="https://metadriverse.github.io/policydissect/"><b>Webpage</b></a> |
                <a class="bar" href="https://github.com/metadriverse/policydissect"><b>Code</b></a> |
                <a class="bar" href="https://youtu.be/2Shqhwgom3A"><b>Video</b></a> |
                <!--                                <a class="bar" href="#talk"><b>Talk</b></a> |-->

                <!--                                <a class="bar" href="https://github.com/decisionforce/HACO/blob/main/docs/iclr_poster.pdf"><b>Poster</b></a>-->
                <a class="bar" href="https://arxiv.org/pdf/2206.00152.pdf"><b>Paper</b></a>


            </td>
        </tr>
    </table>
    <!--    <div class="video-container">-->
    <center>
        <video class="video-container" width="95%" height="440" style="padding-left: 15pt" autoplay muted loop id="teaser_video">
            <source src="assets/teaser_video.mp4" type=video/mp4>
        </video>
<!--        <script>-->
<!--            document.getElementById('teaser_video').play();-->
<!--        </script>-->
    </center>
    <!--    </div>-->
</div>


<!-- === Overview Section Starts === -->
<div class="section">
    <div class="body">
        <div class="teaser">
            <img src="assets/overview.png">
            <div class="text">
                <br>
                Fig. 1 Overview of the proposed method
            </div>
        </div>
        <div class="text">

            <p>
                Inspired by the neuroscience approach to investigate the motor cortex in primates<a
                    href="ref-1"><sup>1</sup></a>, we
                develop a simple
                yet effective frequency-based approach called <em>Policy Dissection </em> to align the intermediate
                representation of the learned neural controller
                with the kinematic attributes of the agent behavior. Without modifying the neural controller or
                retraining the model, the proposed approach can convert a given RL-trained policy into a
                goal-conditioned policy, where specific units can be activated to evoke desired behaviors and complete
                goals.
                This, in turn, enables Human-AI shared control where human can control the trained AI and finish complex
                tasks.
            </p>
            <div class="text" style="font-size: 10pt;" id="ref-1">
                <p>
                    1. <a
                        href="https://reader.elsevier.com/reader/sd/pii/S0896627302010036?token=EE38E077CD5A6737DC1EEDF1309838A9B609E4A47110E15E53C9B42F1EA173094A37ACE7622CFC4B43C21E049E63B947&originRegion=eu-west-1&originCreation=20220920182935">
                    <em>Graziano,
                        Michael SA, et al. "The cortical control of movement revisited." Neuron 36.3 (2002):
                        349-362.</em>
                </a>

                </p>
            </div>
        </div>
    </div>
</div>


<!--&lt;!&ndash; === Result Section Starts === &ndash;&gt;-->
<!--<div class="section">-->
<!--    <div class="title" id="lang">Experiment Result</div>-->
<!--    <div class="body">-->
<!--                <div class="teaser"><img-->
<!--                src="images/haco_exp.jpg">-->
<!--                        <div class="text">-->
<!--                <br>-->
<!--                Fig. 2 Learning Dynamics </div>-->
<!--        </div>-->
<!--        <div class="text">-->
<!--            The experiments conducted on <a href="https://github.com/decisionforce/metadrive">MetaDrive Simulator</a>-->
<!--            show the efficient training as well as the low safety violation, when compared with RL, Offline RL,-->
<!--            Imitation Learning and human-in-the-loop baselines. Results are reported in the Table 1.-->
<!--            Compared to other data-hungry baselines, HACO utilizes less transitions and achieve highest success rate.-->
<!--            Also, with the human protection, HACO yields only 30.14 total safety violations in the whole-->
<!--            training process, two orders of magnitude less than other RL baselines, even though HACO access neither-->
<!--            the cost or reward signal.-->
<!--            In the human-in-the-loop paradigm, another concern is the expensive human budget consuming.-->
<!--            As shown in Fig. 2 <b>B</b>., expensive human budget decreases along with the training.-->
<!--        </div>-->
<!--        <div class="teaser">-->
<!--                     <div class="text">-->

<!--                Table. 1 Comparison results-->
<!--                          <br>-->
<!--                     </div>-->
<!--            <img-->
<!--                src="images/exp_table.png">-->
<!--        </div>-->

<!--        </div>-->
<!--    </div>-->
<!--</div>-->
<!--&lt;!&ndash; === Result Section Ends === &ndash;&gt;-->
<div class="section" style=" text-align: left">
    <div class="title" id="Demo Video">Demo Video</div>
    We provide a demo video to show human-AI shared control systems empowered by <em>Policy Dissection</em>
    can be built on various tasks, including quadrupedal robot locomotion, autonomous driving and classic gym tasks.
    <div class="body">
        <div class="video-container" style="position: relative; padding-top: 2%; margin: 0pt auto; text-align: center;">
            <iframe width="900" height="506" src="https://www.youtube.com/embed/2Shqhwgom3A"
                    title="YouTube video player" frameborder="0"
                    allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"
                    allowfullscreen></iframe>
        </div>
    </div>
</div>

<div class="section" style=" text-align: left">
    <div class="title" id="Parkour Demo">Parkour Demo</div>
    <p>
        We trained bipedal robots, Cassie, in <a href="https://github.com/NVIDIA-Omniverse/IsaacGymEnvs">IsaacGym </a>.
        Though this robot is trained to move forward, activating identified primitives can evoke complex behaviors
        like crouching, forward jumping and back-flipping. In the following parkour video, we show that with human's
        instruction, the robot can combine these skills and overcome complex situations.
        This neural controller with probed primitives and the shared control interface are all released <a href="https://github.com/metadriverse/policydissect"> here </a>.
    </p>
    <div class="body">
        <div class="video-container" style="position: relative; padding-top: 2%; margin: 0pt auto; text-align: center;">
            <iframe width="900" height="506" src="https://www.youtube.com/embed/cqCmy-aNuD4"
                    title="YouTube video player" frameborder="0"
                    allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"
                    allowfullscreen
            ></iframe>
        </div>
    </div>
</div>


<div class="section" style=" text-align: left">
    <div class="title" id="Tracking Demo">Comparison with goal-conditioned controller</div>
    <p>
        To quantify the coarseness of the goal-conditioned control enabled by <em>Policy Dissection</em>,
        a explicit goal-conditioned controller following a target yaw is trained in <a
            href="https://github.com/NVIDIA-Omniverse/IsaacGymEnvs">IsaacGym </a>.
        We directly identify the primitives related to yaw rate in this controller with the proposed method, and employ
        a PID controller to determine the output of the neuron related to yaw rate. This enables a new way,
        neural primitive activation, to track target command besides explicitly indicating the goal in the network
        input.
    </p>
    <p>
        Consequently, experiments can be conducted to fairly compare these two methods for quantifying the
        coarseness of <em>Policy Dissection</em> enabled goal-conditioned control.
        As shown in the video, the tracking precision of the goal-conditioned controller achieved by our method is compatible to the
        explict goal-conditioned control method.
        <br><br><em><b>Note</b>: the explict target yaw command in the observation is set to 0 for the primitive
        activation command tracking.</em>
    </p>
    <div class="body">
        <div class="video-container" style="position: relative; padding-top: 2%; margin: 0pt auto; text-align: center;">
            <iframe width="900" height="506" src="https://www.youtube.com/embed/T6qg3IYlc2A"
                    title="YouTube video player" frameborder="0"
                    allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"
                    allowfullscreen></iframe>
        </div>
    </div>
</div>

<!--<div class="section">-->
<!--    <div class="title" id="talk">Talk</div>-->
<!--    <div class="text">-->
<!--        <br>-->
<!--        <p>-->
<!--            We summarize our core technical comtribution in this talk.-->
<!--        </p>-->
<!--    </div>-->
<!--    <div class="body">-->
<!--        <div class="vedio" style="position: relative; padding-top: 2%; margin: 0pt auto; text-align: center;">-->
<!--            <iframe width="900" height="506" src="https://www.youtube.com/embed/PiJv4wtp8T8"-->
<!--                    title="YouTube video player" frameborder="0"-->
<!--                    allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"-->
<!--                    allowfullscreen></iframe>-->
<!--        </div>-->
<!--    </div>-->
<!--    </div>-->


<!--<div class="section">-->
<!--    <div class="title" id="video">Demo Video</div>-->
<!--    <div class="text">-->
<!--        <br>-->
<!--        <p>-->
<!--            HACO is tested on <a href="https://github.com/decisionforce/metadrive">MetaDrive Simulator</a>, which is-->
<!--            efficient and allows generating various scenarios. Here, we provide the full training process of HACO and-->
<!--            compare it with RL, IL and Offline RL baselines. As a result, HACO achieves superior sample efficiency-->
<!--            with safety guarantee and outperform all baselines.-->
<!--        </p>-->
<!--    </div>-->
<!--    <div class="body">-->
<!--        <div class="vedio" style="position: relative; padding-top: 2%; margin: 0pt auto; text-align: center;">-->
<!--            <iframe width="900" height="506" src="https://www.youtube.com/embed/Mp37zErSXwk"-->
<!--                    title="YouTube video player" frameborder="0"-->
<!--                    allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"-->
<!--                    allowfullscreen></iframe>-->
<!--        </div>-->
<!--    </div>-->
<!--    </div>-->
<!--<div class="section">-->
<!--    <div class="title" id="video_carla">Benchmark HACO on CARLA</div>-->
<!--    <div class="text">-->
<!--        <br>-->
<!--        <p>-->
<!--            Furthermore, we benchmark HACO on <a href=https://github.com/carla-simulator/carla>CARLA Simulator</a>,-->
<!--            where agent takes semantic top-down view as observation. Equipped with 3-layer convolution neural network,-->
<!--            HACO agent learns not only the feature extractor but the driving policy with human involvement in 10 minutes.-->
<!--        </p>-->
<!--    </div>-->
<!--    <div class="body">-->
<!--        <div class="vedio" style="position: relative; padding-top: 2%; margin: 0pt auto; text-align: center;">-->
<!--            <iframe width="900" height="506" src="https://www.youtube.com/embed/vm-Cog-9feY"-->
<!--                    title="YouTube video player" frameborder="0"-->
<!--                    allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"-->
<!--                    allowfullscreen></iframe>-->
<!--        </div>-->
<!--    </div>-->
<!--</div>-->


<!-- === Reference Section Starts === -->
<div class="section">
    <div class="bibtex">
        <div class="text">Reference</div>
    </div>
    <!--    If you find this work useful in your project, please consider to cite it through:-->
    <pre>
 @article{li2022human,
  title={Human-AI Shared Control via Policy Dissection},
  author={Li, Quanyi and Peng, Zhenghao and Wu, Haibin and Feng, Lan and Zhou, Bolei},
  journal={arXiv preprint arXiv:2206.00152},
  year={2022}}
    </pre>
    <!-- Adjust the frame size based on the demo (Every project differs). -->
</div>

</body>
</html>
